{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk2BIjGsIDz7"
      },
      "outputs": [],
      "source": [
        "%pip install optimum\n",
        "%pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
        "%pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/  # Use cu118 if on CUDA 11.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8gGIM_tJaCm"
      },
      "outputs": [],
      "source": [
        "# @title Load the model: Mistral7B-instruct-v0.1\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "\n",
        "model_name_or_path = 'TheBloke/Mistral-7B-Instruct-v0.1-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map='auto',\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision='main')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxFDU3mhWklV",
        "outputId": "4beba777-7077-4cd3-c12c-aea8530fe875"
      },
      "outputs": [],
      "source": [
        "# @title Ask it anything\n",
        "prompt = 'Schrijf een gedicht over incompetente managers die zich verschuilen achter moeders rok. In hey Nederlands'\n",
        "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
        "'''\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "\n",
        "output = model.generate(input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=2048, repetition_penalty=1.1, streamer=streamer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
