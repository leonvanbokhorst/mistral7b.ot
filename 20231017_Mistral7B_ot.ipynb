{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk2BIjGsIDz7"
      },
      "outputs": [],
      "source": [
        "%pip install optimum\n",
        "%pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
        "%pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu118 if on CUDA 11.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f8gGIM_tJaCm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\post\\miniconda3\\envs\\textgen\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bin c:\\Users\\post\\miniconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "# @title Load the model: Mistral7B-instruct-v0.1\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "\n",
        "model_name_or_path = 'TheBloke/Mistral-7B-Instruct-v0.1-GPTQ'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map='auto',\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision='main')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxFDU3mhWklV",
        "outputId": "4beba777-7077-4cd3-c12c-aea8530fe875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ik vind het niet alleen grappig,\n",
            "Wanneer managers geen klap hebben\n",
            "Om hun plannen te vervangen,\n",
            "En zich schuilen achter mama's rok.\n",
            "\n",
            "Ze weten niet wat ze doen,\n",
            "Niet waar ze gaan, noch wie ze beloven,\n",
            "En het geheim van hun succes,\n",
            "Is dat ze een beetje mee draaien.\n",
            "\n",
            "Ze zouden kunnen leren om hun plannen te bedenken,\n",
            "Zonder de moeder met de broek te pakken,\n",
            "Kun je er een goede manager voor,\n",
            "Een persoon die de weg kan vervolgen.\n",
            "\n",
            "Bijvoorbeeld mama heeft veel ervaring,\n",
            "Maar zonder haar planners,\n",
            "Het is niet mogelijk,\n",
            "Tot het klaar is geworden.\n",
            "\n",
            "Als je een goede manager wilt hebben,\n",
            "Kun je de moeder voorbij laten,\n",
            "De klap geven en het eigen werk doen,\n",
            "Een goede manager weet wat er aan te doen.</s>\n"
          ]
        }
      ],
      "source": [
        "# @title Ask it anything\n",
        "prompt = 'Schrijf een gedicht over incompetente managers die zich verschuilen achter moeders rok. In hey Nederlands'\n",
        "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
        "'''\n",
        "\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "\n",
        "output = model.generate(input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=2048, repetition_penalty=1.1, streamer=streamer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
